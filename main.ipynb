{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from CNC_Machining.utils import data_loader_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "## if you're on M1 or M2 GPU:\n",
    "# device = torch.device(\"mps\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CNC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "machines = [\"M01\",\"M02\",\"M03\"]\n",
    "process_names = [\"OP00\",\"OP01\",\"OP02\",\"OP03\",\"OP04\",\"OP05\",\"OP06\",\"OP07\",\"OP08\",\"OP09\",\"OP10\",\"OP11\",\"OP12\",\"OP13\",\"OP14\"]\n",
    "labels = [\"good\",\"bad\"]\n",
    "path_to_dataset = \"./CNC_Machining/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laoding files from ./CNC_Machining/data/M01/OP00/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP00/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP00/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP00/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP00/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP00/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP01/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP01/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP01/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP01/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP01/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP01/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP02/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP02/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP02/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP02/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP02/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP02/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP03/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP03/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP03/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP03/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP03/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP03/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP04/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP04/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP04/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP04/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP04/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP04/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP05/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP05/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP05/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP05/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP05/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP05/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP06/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP06/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP06/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP06/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP06/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP06/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP07/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP07/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP07/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP07/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP07/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP07/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP08/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP08/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP08/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP08/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP08/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP08/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP09/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP09/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP09/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP09/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP09/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP09/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP10/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP10/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP10/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP10/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP10/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP10/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP11/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP11/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP11/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP11/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP11/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP11/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP12/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP12/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP12/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP12/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP12/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP12/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP13/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP13/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP13/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP13/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP13/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP13/bad... \n",
      "laoding files from ./CNC_Machining/data/M01/OP14/good... \n",
      "laoding files from ./CNC_Machining/data/M01/OP14/bad... \n",
      "laoding files from ./CNC_Machining/data/M02/OP14/good... \n",
      "laoding files from ./CNC_Machining/data/M02/OP14/bad... \n",
      "laoding files from ./CNC_Machining/data/M03/OP14/good... \n",
      "laoding files from ./CNC_Machining/data/M03/OP14/bad... \n"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "class_count = {}\n",
    "\n",
    "# To avoid problems with firing rate we can rectify the signal and normalize the max\n",
    "\n",
    "dataset_max = 0\n",
    "for process_name in process_names:\n",
    "    for machine in machines:\n",
    "        for label in labels:\n",
    "            data_path = os.path.join(path_to_dataset, machine, process_name, label)\n",
    "            data_list, data_label = data_loader_utils.load_tool_research_data(data_path, label=label)\n",
    "            \n",
    "            # Rectify \n",
    "            for data_i, data in enumerate(data_list):\n",
    "                data = data-np.mean(data,axis=0)\n",
    "                data[data<0] = -data[data<0]\n",
    "                data_list[data_i] = data\n",
    "                if np.max(np.abs(data))>dataset_max:\n",
    "                    dataset_max = np.max(np.abs(data))\n",
    "                \n",
    "            #concatenating\n",
    "            X_data.extend(data_list)\n",
    "            y_data.extend(data_label)\n",
    "\n",
    "n_recs = len(y_data)\n",
    "\n",
    "cnc_data_info_type = [('machine', 'U3'), ('process', 'U4'), ('class', 'U4'), ('rec_date', 'U10'), ('idx', 'U4'), ('n_samples', np.int32)]\n",
    "cnc_data_info = np.zeros(n_recs,dtype=cnc_data_info_type)            \n",
    "            \n",
    "for label_i, label in enumerate(y_data):\n",
    "    d_machine = label[:3]\n",
    "    d_process = label[13:17]\n",
    "    d_class = label[22:]\n",
    "    d_rec_date = label[4:12]\n",
    "    d_idx = label[18:21]\n",
    "    d_n_samples = len(X_data[label_i])\n",
    "    cnc_data_info[label_i] = np.array([(d_machine, d_process, d_class, d_rec_date, d_idx, d_n_samples)], dtype=cnc_data_info_type)\n",
    "\n",
    "class_count['good'] = sum(cnc_data_info['class']=='good')\n",
    "class_count['bad'] = sum(cnc_data_info['class']=='bad')\n",
    "\n",
    "# Normalize data\n",
    "for data_i, data in enumerate(X_data):\n",
    "    X_data[data_i] = data/dataset_max \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file= path_to_dataset + \"M01/OP00/good/M01_Aug_2019_OP00_000.h5\"\n",
    "sample = data_loader_utils.datafile_read(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 1632, 'bad': 70}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count #the dataset is unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a balanced dataset subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of matched recordings: 122\n",
      "Train single batch size: 2\n",
      "Test single batch size: 2\n"
     ]
    }
   ],
   "source": [
    "bad_subset_idx = cnc_data_info['class']=='bad'\n",
    "good_subset_idx = cnc_data_info['class']=='good'\n",
    "bad_subset = cnc_data_info[bad_subset_idx]\n",
    "dataset_idx = []\n",
    "\n",
    "n_axis = 2 #Choose the number of axis to represent\n",
    "train_batches_number = 2\n",
    "\n",
    "for bad_rec_i, bad_rec in enumerate(bad_subset):\n",
    "    good_match_idx = np.ones(n_recs, dtype=np.int32)\n",
    "    good_match_idx = good_match_idx*(bad_rec['machine'] == cnc_data_info['machine'])\n",
    "    good_match_idx = good_match_idx*(bad_rec['process'] == cnc_data_info['process'])\n",
    "    good_match_idx = good_match_idx*(bad_rec['rec_date'] == cnc_data_info['rec_date'])\n",
    "    good_match_idx = good_match_idx*(bad_rec['idx'] == cnc_data_info['idx'])\n",
    "    bad_match_idx = good_match_idx*bad_subset_idx\n",
    "    good_match_idx = good_match_idx*good_subset_idx\n",
    "    bad_match_idx = np.where(bad_match_idx==1)[0]\n",
    "    good_match_idx = np.where(good_match_idx==1)[0]\n",
    "    if good_match_idx.size > 0:\n",
    "        dataset_idx.append(good_match_idx[0])\n",
    "        dataset_idx.append(bad_match_idx[0]) \n",
    "    \n",
    "dataset_idx = np.array(dataset_idx)\n",
    "print(\"Total number of matched recordings: \"+str(len(dataset_idx)))\n",
    "\n",
    "dataset_idx_train_good = []\n",
    "dataset_idx_train_bad = []\n",
    "dataset_idx_test_good = []\n",
    "dataset_idx_test_bad = []\n",
    "\n",
    "for idx in dataset_idx:\n",
    "    \n",
    "    info = cnc_data_info[idx]\n",
    "    \n",
    "    if (info['machine']=='M01') and (info['process']=='OP05') and (info['rec_date']=='Feb_2019'):\n",
    "        # Split the training set and the test set along idx\n",
    "        if info['idx']=='000':\n",
    "            \n",
    "            if info['class']=='good':\n",
    "                dataset_idx_train_good.append(idx)\n",
    "            else:\n",
    "                dataset_idx_train_bad.append(idx)\n",
    "        else:\n",
    "            \n",
    "            if info['class']=='good':\n",
    "                dataset_idx_test_good.append(idx)\n",
    "            else:\n",
    "                dataset_idx_test_bad.append(idx)\n",
    "        \n",
    "train_idx = dataset_idx_train_good + dataset_idx_train_bad\n",
    "test_idx = dataset_idx_test_good + dataset_idx_test_bad\n",
    "\n",
    "train_batch_size = len(train_idx)\n",
    "test_batch_size = len(test_idx) \n",
    "print(\"Train single batch size: \"+str(train_batch_size))\n",
    "print(\"Test single batch size: \"+str(test_batch_size))       \n",
    "min_train_samples = min(cnc_data_info[train_idx]['n_samples'])\n",
    "min_test_samples = min(cnc_data_info[test_idx]['n_samples'])\n",
    "\n",
    "#Create the actual train and test set by batching batch_i*n_samples*batch_size*3(IMUdim)\n",
    "#train_data = np.zeros([train_batch_size, min_train_samples,1,n_axis])\n",
    "#test_data = np.zeros([test_batch_size, min_test_samples,1,n_axis])\n",
    "train_data = np.zeros([min_train_samples,train_batch_size,n_axis])\n",
    "test_data = np.zeros([min_test_samples,test_batch_size,n_axis])\n",
    "\n",
    "train_labels = 1*(cnc_data_info[train_idx]['class']=='bad')\n",
    "test_labels = 1*(cnc_data_info[test_idx]['class']=='bad')\n",
    "\n",
    "for j, train_i in enumerate(train_idx):\n",
    "    #train_data[j,:,0] = X_data[train_i][:min_train_samples,:n_axis]\n",
    "    train_data[:,j] = X_data[train_i][:min_train_samples,:n_axis]\n",
    "    \n",
    "for j, test_i in enumerate(test_idx):\n",
    "    #test_data[j,:,0] = X_data[test_i][:min_test_samples,:n_axis]\n",
    "    test_data[:,j] = X_data[test_i][:min_test_samples,:n_axis]\n",
    "    \n",
    "train_data = torch.tensor(train_data, dtype=dtype)\n",
    "train_labels = (F.one_hot(torch.tensor(train_labels)).float())\n",
    "test_data = torch.tensor(test_data, dtype=dtype)\n",
    "test_labels =  (F.one_hot(torch.tensor(test_labels)).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('M01', 'OP05', 'good', 'Feb_2019', '000', 40800),\n",
       "       ('M01', 'OP05', 'bad', 'Feb_2019', '000', 29831)],\n",
       "      dtype=[('machine', '<U3'), ('process', '<U4'), ('class', '<U4'), ('rec_date', '<U10'), ('idx', '<U4'), ('n_samples', '<i4')])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_info[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('M01', 'OP05', 'good', 'Feb_2019', '001', 40800),\n",
       "       ('M01', 'OP05', 'bad', 'Feb_2019', '001', 29621)],\n",
       "      dtype=[('machine', '<U3'), ('process', '<U4'), ('class', '<U4'), ('rec_date', '<U10'), ('idx', '<U4'), ('n_samples', '<i4')])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_info[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Neuron Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plotting Settings\n",
    "def plot_cur_mem_spk(cur, mem, spk, thr_line=False, vline=False, title=False, ylim_max1=1.25, ylim_max2=1.25):\n",
    "  # Generate Plots\n",
    "  fig, ax = plt.subplots(3, figsize=(8,6), sharex=True, \n",
    "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
    "\n",
    "  # Plot input current\n",
    "  ax[0].plot(cur, c=\"tab:orange\")\n",
    "  ax[0].set_ylim([0, ylim_max1])\n",
    "  ax[0].set_xlim([0, 200])\n",
    "  ax[0].set_ylabel(\"Input Current ()\")\n",
    "  if title:\n",
    "    ax[0].set_title(title)\n",
    "\n",
    "  # Plot membrane potential\n",
    "  ax[1].plot(mem)\n",
    "  ax[1].set_ylim([0, ylim_max2]) \n",
    "  ax[1].set_ylabel(\"Membrane Potential ()\")\n",
    "  if thr_line:\n",
    "    ax[1].axhline(y=thr_line, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
    "  plt.xlabel(\"Time step\")\n",
    "\n",
    "  # Plot output spike using spikeplot\n",
    "  splt.raster(spk, ax[2], s=400, c=\"black\", marker=\"|\")\n",
    "  if vline:\n",
    "    ax[2].axvline(x=vline, ymin=0, ymax=6.75, alpha = 0.15, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
    "  plt.ylabel(\"Output spikes\")\n",
    "  plt.yticks([]) \n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def plot_snn_spikes(spk_in, spk1_rec, spk2_rec, title):\n",
    "  # Generate Plots\n",
    "  fig, ax = plt.subplots(3, figsize=(8,7), sharex=True, \n",
    "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
    "\n",
    "  # Plot input spikes\n",
    "  splt.raster(spk_in[:,0], ax[0], s=0.03, c=\"black\")\n",
    "  ax[0].set_ylabel(\"Input Spikes\")\n",
    "  ax[0].set_title(title)\n",
    "\n",
    "  # Plot hidden layer spikes\n",
    "  splt.raster(spk1_rec.reshape(num_steps, -1), ax[1], s = 0.05, c=\"black\")\n",
    "  ax[1].set_ylabel(\"Hidden Layer\")\n",
    "\n",
    "  # Plot output spikes\n",
    "  splt.raster(spk2_rec.reshape(num_steps, -1), ax[2], c=\"black\", marker=\"|\")\n",
    "  ax[2].set_ylabel(\"Output Spikes\")\n",
    "  ax[2].set_ylim([0, 10])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def dvs_animator(spike_data):\n",
    "  fig, ax = plt.subplots()\n",
    "  anim = splt.animator((spike_data[:,0] + spike_data[:,1]), fig, ax)\n",
    "  return anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_test = 0.8\n",
    "threshold_test = 0.1\n",
    "lif = snn.Leaky(beta=beta_test, threshold=threshold_test) # LIF neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup inputs\n",
    "num_steps = 200 # number of time-steps to simulate\n",
    "\n",
    "x = train_data[:,0,:1].sum(1)\n",
    "\n",
    "# Small step current input\n",
    "# w = 0.15, # then run 0.20, 0.21\n",
    "# x = torch.cat((torch.zeros(10), torch.ones(190)*w), 0)\n",
    "mem = torch.zeros(1)\n",
    "spk = torch.zeros(1)\n",
    "\n",
    "mem_rec = []\n",
    "spk_rec = []\n",
    "\n",
    "# neuron simulation\n",
    "for step in range(num_steps):\n",
    "  spk, mem = lif(x[step], mem)\n",
    "  mem_rec.append(mem)\n",
    "  spk_rec.append(spk)\n",
    "\n",
    "# convert lists to tensors\n",
    "mem_rec = torch.stack(mem_rec)\n",
    "spk_rec = torch.stack(spk_rec)\n",
    "\n",
    "plot_cur_mem_spk(x, mem_rec, spk_rec, thr_line=threshold_test, ylim_max1=1.0,\n",
    "                 title=\"snn.Leaky Neuron Model\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = n_axis\n",
    "num_hidden = 10\n",
    "num_outputs = 2\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = min_train_samples\n",
    "hid_beta = np.linspace(0.4,0.8,num_hidden)\n",
    "hid_beta = torch.tensor(hid_beta, dtype=dtype)\n",
    "out_beta = 0.95\n",
    "hid_thresh = 0.1\n",
    "thresh = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure accuracy \n",
    "def measure_accuracy(spk_rec, targets):\n",
    "  with torch.no_grad():\n",
    "    running_length = 0\n",
    "    running_accuracy = 0\n",
    "    \n",
    "    spike_count = spk_rec.sum(0)\n",
    "    _, max_spike = spike_count.max(1)\n",
    "\n",
    "    # correct classes for one batch\n",
    "    num_correct = (max_spike == torch.argmax(targets, dim=1)).sum()\n",
    "\n",
    "    # total accuracy\n",
    "    running_length += len(targets)\n",
    "    running_accuracy += num_correct\n",
    "\n",
    "    accuracy = (running_accuracy / running_length)\n",
    "\n",
    "    return accuracy.item()\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
    "        self.fc1.weight.data = torch.ones(self.fc1.weight.size())\n",
    "        self.lif1 = snn.Leaky(beta=hid_beta, threshold=hid_thresh)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
    "        self.fc2.weight.data = torch.abs(self.fc2.weight.data)\n",
    "        self.lif2 = snn.Leaky(beta=out_beta, threshold=thresh)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        \n",
    "        # Also part of the first for debug\n",
    "        spk1_rec = []\n",
    "        \n",
    "        num_steps = len(x)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "            spk1_rec.append(spk1)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0), torch.stack(spk1_rec, dim=0)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 11874.8671875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 1 \t Train Loss: 10591.7734375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 2 \t Train Loss: 9754.9755859375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 3 \t Train Loss: 8879.5732421875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 4 \t Train Loss: 8037.115234375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 5 \t Train Loss: 7579.18115234375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 6 \t Train Loss: 7538.36962890625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 7 \t Train Loss: 7517.80078125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 8 \t Train Loss: 7502.6884765625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 9 \t Train Loss: 7497.52587890625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 10 \t Train Loss: 7483.22314453125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 11 \t Train Loss: 7475.23828125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 12 \t Train Loss: 7468.55712890625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 13 \t Train Loss: 7420.482421875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 14 \t Train Loss: 7398.64013671875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 15 \t Train Loss: 7377.0341796875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 16 \t Train Loss: 7350.99560546875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 17 \t Train Loss: 7308.02783203125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 18 \t Train Loss: 7268.88134765625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 19 \t Train Loss: 7229.2412109375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 20 \t Train Loss: 7169.41796875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 21 \t Train Loss: 7102.9873046875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 22 \t Train Loss: 7030.57177734375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 23 \t Train Loss: 6947.95947265625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 24 \t Train Loss: 6864.79638671875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 25 \t Train Loss: 6788.23291015625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 26 \t Train Loss: 6689.24755859375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 27 \t Train Loss: 6652.9091796875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 28 \t Train Loss: 6602.28564453125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 29 \t Train Loss: 6556.8681640625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 30 \t Train Loss: 6535.98779296875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 31 \t Train Loss: 6737.65185546875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 32 \t Train Loss: 8209.87890625\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 33 \t Train Loss: 6675.29345703125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 34 \t Train Loss: 6441.1240234375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 35 \t Train Loss: 6390.435546875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 36 \t Train Loss: 6292.1982421875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 37 \t Train Loss: 6292.28759765625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 38 \t Train Loss: 6204.99462890625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 39 \t Train Loss: 6850.033203125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 40 \t Train Loss: 6453.32958984375\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 41 \t Train Loss: 7334.966796875\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 42 \t Train Loss: 6264.83837890625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 43 \t Train Loss: 6395.408203125\n",
      "Train set accuracy: 0.5\n",
      "Iteration: 44 \t Train Loss: 6124.69384765625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 45 \t Train Loss: 6168.6171875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 46 \t Train Loss: 6014.26416015625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 47 \t Train Loss: 6042.4306640625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 48 \t Train Loss: 5961.708984375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 49 \t Train Loss: 5955.95166015625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 50 \t Train Loss: 5929.0771484375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 51 \t Train Loss: 5923.73583984375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 52 \t Train Loss: 5892.6787109375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 53 \t Train Loss: 5894.044921875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 54 \t Train Loss: 5857.66064453125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 55 \t Train Loss: 5840.15185546875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 56 \t Train Loss: 5813.51318359375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 57 \t Train Loss: 5794.34375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 58 \t Train Loss: 5769.9267578125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 59 \t Train Loss: 5746.5771484375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 60 \t Train Loss: 5734.64453125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 61 \t Train Loss: 5718.29345703125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 62 \t Train Loss: 5696.48828125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 63 \t Train Loss: 5659.53515625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 64 \t Train Loss: 5630.83642578125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 65 \t Train Loss: 5615.1044921875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 66 \t Train Loss: 5605.9873046875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 67 \t Train Loss: 5580.16455078125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 68 \t Train Loss: 5555.88916015625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 69 \t Train Loss: 5535.43212890625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 70 \t Train Loss: 5504.10595703125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 71 \t Train Loss: 5463.33447265625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 72 \t Train Loss: 5413.8203125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 73 \t Train Loss: 5386.9287109375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 74 \t Train Loss: 5368.81103515625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 75 \t Train Loss: 5345.837890625\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 76 \t Train Loss: 5305.27685546875\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 77 \t Train Loss: 5274.3330078125\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 78 \t Train Loss: 5221.4130859375\n",
      "Train set accuracy: 1.0\n",
      "Iteration: 79 \t Train Loss: 5165.7412109375\n",
      "Train set accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import snntorch.functional as SF\n",
    "\n",
    "loss = SF.mse_count_loss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "num_epochs = 80\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "data = train_data.to(device)\n",
    "targets = train_labels.to(device)\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    # forward pass\n",
    "    net.train()\n",
    "    spk_rec, _, spk_rec_hid  = net(data)\n",
    "\n",
    "    # initialize the loss & sum over time\n",
    "    loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "    loss_val += loss(spk_rec, torch.argmax(targets,axis=1))\n",
    "\n",
    "    # Gradient calculation + weight update\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store loss history for future plotting\n",
    "    loss_hist.append(loss_val.item())\n",
    "\n",
    "    # Print train/test loss/accuracy\n",
    "    print(f\"Iteration: {epoch} \\t Train Loss: {loss_val.item()}\")\n",
    "    print(f\"Train set accuracy: {measure_accuracy(spk_rec, targets)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def measure_test_accuracy(model, test_data, test_targets):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_length = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    data = test_data.to(device)\n",
    "    targets = test_targets.to(device)\n",
    "\n",
    "    # forward-pass\n",
    "    spk_rec, _, spk_rec_hid = model(data)\n",
    "    spike_count = spk_rec.sum(0)\n",
    "    _, max_spike = spike_count.max(1)\n",
    "\n",
    "    # correct classes for one batch\n",
    "    num_correct = (max_spike == targets).sum()\n",
    "\n",
    "    # total accuracy\n",
    "    running_length += len(targets)\n",
    "    running_accuracy += num_correct\n",
    "\n",
    "    accuracy = (running_accuracy / running_length)\n",
    "\n",
    "    return accuracy.item()\n",
    "\n",
    "print(f\"Test Accuracy: {measure_test_accuracy(net, test_data, test_labels)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
